{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0afc35c6-1e26-4862-bd45-13417b07594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------------+------------------+\n",
      "| Test Name | Status | Executed in (sec) | Memory used (Mb) |\n",
      "+===========+========+===================+==================+\n",
      "|  test.0   | PASSED |     0.000018      |    90.628906     |\n",
      "+-----------+--------+-------------------+------------------+\n",
      "|  test.1   | PASSED |     0.000065      |    90.628906     |\n",
      "+-----------+--------+-------------------+------------------+\n",
      "|  test.2   | PASSED |     0.000060      |    90.628906     |\n",
      "+-----------+--------+-------------------+------------------+\n",
      "|  test.3   | PASSED |     0.000064      |    90.628906     |\n",
      "+-----------+--------+-------------------+------------------+\n",
      "|  test.4   | FAILED |     0.000071      |    90.628906     |\n",
      "+-----------+--------+-------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "\n",
    "from memory_profiler import memory_usage\n",
    "from texttable import Texttable\n",
    "\n",
    "\n",
    "dir_name = 'test_data/'\n",
    "\n",
    "\n",
    "class TestCase:\n",
    "    \n",
    "    def __init__(self, name: str, in_file: str, out_file: str):\n",
    "        self.name = name\n",
    "        self.in_data = self.read_file_plain(in_file)\n",
    "        self.out_data = self.read_file_plain(out_file)\n",
    "        \n",
    "    def read_file_plain(self, filename):\n",
    "        f = open(filename, \"r\")\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            return lines[0].strip()\n",
    "        else:\n",
    "            raise ReadFileError(f\"File named {filename} is empty!\")\n",
    "    \n",
    "    def _run_test(self, testing_func: Callable, args, kwargs):\n",
    "        args = (self.in_data, *args)\n",
    "        real_result = testing_func(*args, **kwargs)\n",
    "        test_result = {\n",
    "            \"test_name\": self.name,\n",
    "            \"is_passed\": \"PASSED\" if real_result == self.out_data else \"FAILED\"\n",
    "        }\n",
    "        return test_result\n",
    "    \n",
    "    def _gather_metrics(self, testing_func: Callable, args, kwargs):\n",
    "        start = time.time()\n",
    "        result = self._run_test(testing_func, args, kwargs)\n",
    "        end = time.time()\n",
    "        _metric_execution_time = (\"{:.6f}\".format(end - start))\n",
    "        result[\"execution_time\"] = _metric_execution_time\n",
    "        \n",
    "        inaccurate_mem_usage = memory_usage((testing_func, (self.in_data, *args), kwargs))\n",
    "        inaccurate_mem_usage = sum(inaccurate_mem_usage)/len(inaccurate_mem_usage)\n",
    "        _metric_rude_mem_usage = (\"{:.6f}\".format(inaccurate_mem_usage))\n",
    "        result[\"rude_mem_usage\"] = _metric_rude_mem_usage\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run(self, testing_func: Callable, func_args, func_kwargs):\n",
    "        return self._gather_metrics(testing_func, func_args, func_kwargs)\n",
    "\n",
    "\n",
    "class TestRunner:\n",
    "    \n",
    "    def __init__(self, dir_name=\"test_data/\"):\n",
    "        self.data_dir = dir_name\n",
    "        # self.validate_data_existance() unimplemented yet; TODO: validate if there are needed dir & it's not empty\n",
    "        self.test_case_class = TestCase\n",
    "        self.test_case_collection = self.build_test_case_collection()\n",
    "        self.full_run_results = None\n",
    "    \n",
    "    def build_test_case_collection(self):\n",
    "        test_collection = []\n",
    "\n",
    "        tmp_collection = self.gather_tmp_test_data()\n",
    "\n",
    "        for test_name in sorted(tmp_collection.keys()):\n",
    "            test_case = self.test_case_class(\n",
    "                test_name,\n",
    "                self.data_dir + tmp_collection[test_name][\"in\"],\n",
    "                self.data_dir + tmp_collection[test_name][\"out\"]\n",
    "            )\n",
    "            test_collection.append(test_case)\n",
    "\n",
    "        return test_collection\n",
    "\n",
    "    def gather_tmp_test_data(self) -> dict:\n",
    "        is_test_file = lambda x: x if (x.endswith(\".in\") or x.endswith(\".out\")) else False\n",
    "        test_names = filter(lambda x: is_test_file(x), os.listdir(self.data_dir))\n",
    "        test_names = list(set(map(lambda x: x[:-3] if x.endswith(\".in\") else x[:-4], test_names)))\n",
    "        tmp_collection = {test_name: {} for test_name in test_names}\n",
    "\n",
    "        for filename in os.listdir(self.data_dir):\n",
    "            if filename.endswith(\".in\"):\n",
    "                test_name = filename[:-3]\n",
    "                tmp_collection[test_name][\"in\"] = filename\n",
    "            elif filename.endswith(\".out\"):\n",
    "                test_name = filename[:-4]\n",
    "                tmp_collection[test_name][\"out\"] = filename\n",
    "\n",
    "        return tmp_collection\n",
    "    \n",
    "    def run_tests(self, func_to_test: Callable, func_args=None, func_kwargs=None):\n",
    "        func_args = func_args or ()\n",
    "        func_kwargs = func_kwargs or {}\n",
    "        test_results = []\n",
    "        for test_case in self.test_case_collection:\n",
    "            test_result = test_case.run(func_to_test, func_args, func_kwargs)\n",
    "            test_results.append(self.format_result(test_result))\n",
    "\n",
    "        self.full_run_results = test_results\n",
    "        return test_results\n",
    "    \n",
    "    def format_result(self, result):\n",
    "        result_dict = {\n",
    "            \"result_datetime\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "            \"result\": result\n",
    "        }\n",
    "        return result_dict\n",
    "    \n",
    "    def render_results_as_raw_text(self):\n",
    "        all_results_string = \"\"\n",
    "        for result in self.full_run_results:\n",
    "            all_results_string = all_results_string + self.render_result_as_raw_text(result) + \"\\n\\n\"\n",
    "        return all_results_string\n",
    "            \n",
    "    def render_result_as_raw_text(self, result):\n",
    "        test_res = result.get(\"result\")\n",
    "        test_name = \"Test name: \" + test_res[\"test_name\"] + \"\\n\"\n",
    "        test_status = \"Test passed: \" + str(test_res[\"is_passed\"]) + \"\\n\"\n",
    "        exec_time = \"Executed in: \" + test_res[\"execution_time\"] + \" seconds\" + \"\\n\"\n",
    "        rude_memory_usage = \"Memory used on execution: \" + test_res[\"rude_mem_usage\"] + \"\\n\"\n",
    "        return test_name + test_status + exec_time + rude_memory_usage\n",
    "    \n",
    "    def render_as_ascii_table(self) -> str:\n",
    "        table = Texttable()\n",
    "        table.set_cols_align((\"c\", \"c\", \"c\", \"c\"))\n",
    "        table.set_cols_valign((\"m\", \"m\", \"m\", \"m\"))        \n",
    "        rows_to_add = [[\"Test Name\", \"Status\", \"Executed in (sec)\", \"Memory used (Mb)\"],]\n",
    "        for res in self.full_run_results:\n",
    "            test_res = res.get(\"result\")\n",
    "            res_list = [test_res[\"test_name\"], str(test_res[\"is_passed\"]), test_res[\"execution_time\"], test_res[\"rude_mem_usage\"]]\n",
    "            rows_to_add.append(res_list)\n",
    "        table.set_cols_dtype([\"t\", \"t\", \"f\", \"f\"])\n",
    "        table.set_precision(6)\n",
    "        table.add_rows(rows_to_add)\n",
    "        return table.draw()\n",
    "\n",
    "runner = TestRunner()\n",
    "\n",
    "def measure_len(string_to_measure):\n",
    "    return str(len(string_to_measure))\n",
    "\n",
    "runner.run_tests(measure_len,)\n",
    "# print(runner.render_results_as_raw_text())\n",
    "\n",
    "ascii_table_result = runner.render_as_ascii_table()\n",
    "\n",
    "print(ascii_table_result)\n",
    "\n",
    "# TODO:\n",
    "# - Add time measurement +\n",
    "# - *Add memory usage measurement ??? +\n",
    "# - Add multiple run for average results\n",
    "# - Rendering results into pretty ASCII table in console\n",
    "# - Put it into separate lib\n",
    "# - Create simple string-len script\n",
    "# - Import test_lib into string-len script\n",
    "# - Use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7807003a-f223-498e-b6d9-1eb5172ba49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-11-06T17:23:57'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
